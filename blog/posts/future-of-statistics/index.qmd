---
date: "2025-07-06"
layout: post
title: "A re-imagining of how we produce statistics"
categories: [statistics, productivity, public sector]
image: survey_pipeline_dag.svg
execute:
  echo: false
jupyter: python3
---

National statistics matter: they have a direct bearing on everything from government targets, to funding formulae, to the focus of the media, to the UK's debt. But there have been many high-profile errors in the nation's numbers recently, and that's a cause for concern. Furthermore, the recent [Devereux](https://www.gov.uk/government/publications/independent-review-of-the-performance-and-culture-of-the-office-for-national-statistics/independent-review-by-sir-robert-devereux-kcb-june-2025) and [Public Administration and Constitutional Affairs Committee](https://www.ft.com/content/a342f058-1b20-4706-8b62-d408bbf88c22) (PACAC) reviews have found deep, systemic flaws in the UK's current approach.

I'm a [Royal Statistical Society William Guy Lecturer](https://rss.org.uk/policy-campaigns/policy-groups/education-policy-advisory-group/rss-william-guy-lecturers/upcoming-william-guy-lecturers-2025-26/) for 2025-2026, and my topic is "Economic statistics and stupidly smart AI." With my William Guy Lectureship hat on, I've been thinking a lot about how innovation could help with the production of statistics.

**If we were to start from scratch, what would a statistical institute designed for the world of today—and tomorrow—look like?** How would you rebuild for the era of AI and data science and cloud, take on board the best practices in management, and ensure the right talent was in place to deliver for the nation?

This blog post is my personal attempt at answering these questions and giving a rough blueprint for a different model for producing statistics.

## Introduction

### Background

National statistics are critical infrastructure—essential for decisions that will determine the success of the country. However, the current means of their production has been under pressure. To name a few, there have been recent high-profile errors in [trade](https://www.ft.com/content/61679d49-2b87-45ef-8fa2-1b76e393aed4), [productivity](https://www.ons.gov.uk/economy/economicoutputandproductivity/productivitymeasures/datasets/internationalcomparisonsofproductivityfirstestimates)[^a], [GDP](https://www.ft.com/content/a33e505a-b504-46a9-ba08-e1efd8831a69), [time use](https://www.ons.gov.uk/news/statementsandletters/errorsidentifiedinonsonlinetimeusesurveyotusdata), [innovation](https://www.ons.gov.uk/economy/governmentpublicsectorandtaxes/researchanddevelopmentexpenditure/articles/comparisonofonsbusinessenterpriseresearchanddevelopmentstatisticswithhmrcresearchanddevelopmenttaxcreditstatistics/2022-09-29)[^b], [migration](https://www.ft.com/content/41a07d42-2518-4ec9-9220-943268c93bb5), [census](https://osr.statisticsauthority.gov.uk/publication/review-of-statistics-on-gender-identity-based-on-data-collected-as-part-of-the-2021-england-and-wales-census-final-report/), [health](https://www.ft.com/content/197bb24b-01fd-44b1-9ac7-f70ccf987825) and [labour market figures](https://www.ft.com/content/7d3de81f-b845-490b-804f-af0aa4f76fe3).

The media has noticed—here are some recent headlines:

- [Why the UK's official statistics can no longer be trusted](https://www.telegraph.co.uk/news/2025/05/21/office-for-national-statistics-can-no-longer-be-trusted/), Daily Telegraph, 2025/05/21.
- [Faulty data leaves Britain in the dark: The quality of ONS governance and critical statistics needs fixing](https://www.ft.com/content/f2ef0740-6420-400f-8a78-abc02936aaa9), Financial Times, 2025/04/05.
- [Troubled UK statistics agency warns of errors in its growth figures](https://www.theguardian.com/uk-news/2025/mar/21/troubled-uk-statistics-agency-warns-of-errors-in-its-growth-figures), The Guardian, 2025/03/21.
- [The Office for National Statistics must change to fix its data problems](https://www.instituteforgovernment.org.uk/comment/office-national-statistics-fix-data-problems), Institute for Government, 2025/03/12.

[^a]: The materials accompanying the [productivity revision](https://www.ons.gov.uk/economy/economicoutputandproductivity/productivitymeasures/datasets/internationalcomparisonsofproductivityfirstestimates) said "the previous version suggested that the UK’s average output per hour worked growth rate was 5.0% during the coronavirus (COVID-19) period of 2020 and 2021. The corrected version shows that the UK’s average output per hour growth rate was -0.3% over this period.... the previous version suggested that the UK output per hour worked in 2021 (excluding Japan) had the fastest growth of the G7 countries. This has now been corrected to the second slowest. The previous version also suggested that in 2020 the UK had a fall of 12%, which is now corrected to a 1.2% increase."

[^b]: "...the value of expenditure on R&D performed by UK businesses according to ONS' BERD survey were £15.0 billion, £15.6 billion, and £16.1 billion higher in 2018, 2019 and 2020 respectively than previously estimated."

It is not all bleak: there have been incredible developments in UK statistics in recent years. The Covid Infection Survey was a triumph. Though it's early days, [card data shows promise](https://datasciencecampus.ons.gov.uk/estimating-geographical-retail-markets-from-card-spending-data/). The use of scanner data for inflation is perhaps the strongest contemporary example.[^g] Scanner data are created as consumers purchase goods at a supermarket checkout (or online), and they give the product name, volume, and price paid. This is a giant leap compared to field agents travelling around supermarkets with clipboards, who were able to only record the price, and then only for a much smaller number of products. Scanner data make consumer price inflation (CPI) more accurate, give more information on price changes across the economy, and allow more analysis of what products are driving price rises. The adoption of scanner data was an extremely significant step forward for how inflation is measured in the UK.[^f]

[^g]: Though many other nations [got there first](https://www.insee.fr/en/information/5014725?sommaire=5014796): eg the Netherlands (2002), Norway (2005), Switzerland (2008), Sweden (2012), Belgium (2015), Denmark (2016), Iceland (2016), Luxembourg and Italy (2018).

[^f]: This particular example shows that, with the right conditions, substantial improvements in statistics are possible. In this case, teams were empowered to make changes, to acquire new and relevant data, and also had more permissive than normal use of cloud computing. Flexible use of cloud computing was absolutely essential as there is information on over a billion products per month arriving into the system.

It's also fair to say that there have been headwinds in producing good statistics too: one enormous challenge stems from the long-running issue of declining response rates that has afflicted most national statistical agencies [@stedman_end_2019].

However, many other issues have come from simple spreadsheet and copy-and-paste errors, and could have been avoided. Those reviews, Devereux and PACAC, follow many others[^1] and there overarching theme is that the problems with UK statistics go beyond declining response rates.

So I thought it would be fun to imagine what a statistical institute designed from scratch to produce the best quality product possible would look like.

[^1]: Turnbull-King 1999, Allsopp 2004, Smith 2006, Smith 2014, Johnson 2015, Bean 2016, and Lievesley 2024.

### Imagining better

Why is a better way of doing statistics easy to imagine? Apart from the avoidable errors that those reviews pointed to, we just have much better tools across technology and organisational science to bring to bear on the production of quantitative outputs than we have ever had before, and they should give us huge optimism.

Data science, big data, AI, cloud services, better algorithms, faster hardware, innovations in the management of technical and data-heavy organisations; all could fundamentally change and improve how statistics are produced and delivered [@turrell2025cuttingcomplexitydatascience]. Studies have suggested that in regular organisations, the use of data science and big data can improve productivity by as much as 7% [@brynjolfsson_strength_2011;@muller_effect_2018]. That number should surely be higher for tasks that are solely about the collection, processing, and publication of data.

Also, the management of data-first organisations has been the subject of a great deal of improvement and innovation—with firms like [Amazon](https://www.amazon.co.uk/Working-Backwards-Insights-Stories-Secrets/dp/1529033829), Google, [GitLab](https://handbook.gitlab.com/handbook/company/culture/all-remote/asynchronous/), and more developing ways of working that are suited to the kinds of tasks that statistical production also requires: everything from asynchronous working, to a small and highly skilled workforce, to technical career paths, to "single-threaded" leadership. The management and organisational strategies that have been developed in these frontier firms are part of the reason for their success, and there is every reason to think that other data-centred organisations would benefit from trying them.

Let's now look at some concrete ideas for what to do differently across technology and capital, people and skills, and management.

## Technology and capital

Prof Bart van Ark, managing director of the Productivity Institute, asseses that around one-third of the UK's productivity 'puzzle' is a result of low capital per worker. The UK's capital per hour worked lags behind Italy and the Czech Republic [@allas_uks_2025]. If you were redesigning the production of statistics from scratch, you'd want to ensure that top quality technology and capital were in place.

### Use of AI

AI is just a tool, so I almost didn't honour it with its own section. However, it's where some of the most exciting developments could emerge. Here are a few ways it could help:

- detecting mistakes in survey responses. Machine learning and large language models could significantly increase the rate at which this kind of data error is caught.
- flagging outliers. (Similarly to above.)
- interview transcription—even when the language is not English!
- translation to official taxonomies from free text fields [@turrell20226;@soria2025empirical]. This is currently a burdensome process with much manual labour, but large language models can be targeted to, as an example, turn respondents' written job description into official Standard Occupational Classification categories.
- imputation of missing responses in early estimates. Machine learning excels at matrix completion, and that would be more sophisticated than what is commonly used today (eg ratio implementation.)
- *conducting* survey interviews. Yep, you read that right! See @geiecke2024conversations.
- computer vision can extract data from satellite imagery for, say, building starts: Statistics Canada and the US' Census Bureau are both compiling more accurate, more timely statistics using satellite data [@erman_use_2022]
- similarly, computer vision can be applied to street scenes to assess the cleanliness and desirability of high streets
- accessibility enhancement: AI can automatically generate alternative text for visual content, create audio descriptions, and translate content into multiple languages.
- natural language processing can assess news articles for soft indicators of sentiment (that often lead harder indicators) [@kalamara2022making]
- making statistics more accessible to the general public. Large language model-based [retrieval augmented generation](https://www.llamaindex.ai/) is able to answer users' questions based only on certified information, and can link back to its sources. This would allow for a fundamental shift in how people discover and consume statistics. It would also potentially reduce the number of ad hoc requests. My former colleague Andy Banks led [a project demonstrating the principle](https://datasciencecampus.ons.gov.uk/using-large-language-models-llms-to-improve-website-search-experience-with-statschat/) already. 
- help with coding via tools such as GitHub Copilot, Claude Code, and Gemini CLI.
- drafting articles that are simply summaries of data updates.

### Computing and data infrastructure

Hardware based on Unix, which is well-suited to data and code, would be available to staff for local development. High performing hardware saves labour, so is cost effective, but it's also necessary to run large language models and other AI locally—allowing you to quickly try out ideas. Recognising that people's time is usually the biggest cost in an organisation, top-end laptops would be a no-brainer.

The new automated pipelines would be based on secure cloud services, capitalising on the falling cost, increasing functionality, and scalability of the cloud. These products have [typical service level agreements](https://aws.amazon.com/managed-workflows-for-apache-airflow/sla/) of 99.9% uptime. As far as possible, the cloud infrastructure used would be service provider agnostic to prevent vendor lock-in and to keep costs competitive. The architecture and services used would follow the "infrastructure-as-code" principle, which means that it can be programmatically deployed, and easily re-deployed, including—at least in principle—via a different vendor. [Terraform](https://developer.hashicorp.com/terraform) and related technologies will be used for this.

The data architecture used would be be a 'data lakehouse'. This combines the ability to work with unstructured data and structured data in databases with a data catalogue and other metadata, including version histories, all of which is useful and goes beyond a simple database. Some options are [Amazon Sagemaker Lakehouse](https://aws.amazon.com/sagemaker/lakehouse/), [Google's open lakehouse](https://cloud.google.com/blog/products/data-analytics/extending-the-google-data-cloud-lakehouse-architecture), and [Databricks Lakehouse](https://www.databricks.com/product/data-lakehouse). For local development and prototyping, it's possible to use [DuckLake](https://ducklake.select/).

### Code

The computer code used would of course be under version control, and subject to ongoing quality checks, some automatic (eg unit and integration tests.) Tools like [SQLFluff](https://sqlfluff.com/) and [Ruff](https://docs.astral.sh/ruff/) can flag and even auto-correct some problems in database queries and code respectively, while other tools such as [pre-commit](https://pre-commit.com/) and [panoptipy](https://github.com/aeturrell/panoptipy)[^e] provide ways to flag *potential* issues to both developers and managers. Of course, this code would be under version control. There's even a maximalist approach where the code to produce statistics is open source too, so that problems can be found more quickly by more eyeballs.

[^e]: Full disclosure: I wrote this package!

The code would mainly be Python and SQL, for a whole host of reasons.

Python's dominance in data science reflects several compelling advantages that make it the natural choice for modern analytical work. The language benefits from [vast popularity](https://www.tiobe.com/tiobe-index/) across [multiple measures](https://pypl.github.io/PYPL.html), creating substantial positive network externalities, eg rapid bug discovery, the ability of large language models to write it well, and the sheer volume of available free and open source packages that are on [PyPI](https://pypi.org/). Major conferences like NeurIPS and ICML predominantly feature Python-based research, as do [machine learning competitions](https://mlcontests.com/state-of-machine-learning-competitions-2024).

Python is widely taught and used. It's [taught in some UK and US high schools](https://www.bcs.org/policy-and-influence/education/bcs-landscape-review-computing-qualifications-in-the-uk/england-computer-science-gcse-as-and-a-levels/), both state and [private](https://www.isc.co.uk/media-enquiries/isc-blogs/python-becomes-a-modern-language-for-putney-high-school-s-next-generation-of-digital-nomads/), meaning that the future pipeline of talent is assured. In the professional sphere, [data scientists in the private sector predominantly work in Python and SQL](https://www.kdnuggets.com/2019/05/poll-top-data-science-machine-learning-platforms.html), with this share growing over time. Beyond the academy, firms that sponsor and support Python include [Microsoft, Google, Meta, and Bloomberg](https://www.python.org/psf/sponsors/). Even [RStudio's rebranding to 'Posit'](https://www.infoworld.com/article/3668252/rstudio-changes-name-to-posit-expands-focus-to-include-python-and-vs-code.html) reflects a strategic pivot toward Python support.

Cloud infrastructure considerations are increasingly crucial: services like [Google Cloud Run](https://cloud.google.com/run) and [Azure Functions](https://learn.microsoft.com/en-GB/azure/azure-functions/supported-languages?tabs=isolated-process%2Cv4&pivots=programming-language-csharp) support Python natively.

Just as Python is dominant in the space of analytical code, SQL dominates in database languages. SQL complements Python perfectly, offering flexibility, simplicity compared to distributed databases, wide adoption, and impressive scalability (as demonstrated by Google BigQuery's massive-scale SQL operations).

Using Python and SQL means being able to piggy-back on the wider innovations in these languages, and being able to attract staff for whom statistical production may not be their only ever job.

### Automated data pipelines based on code and focused on quality {#sec-pipelines}

Data pipelines—from ingestion to prepared outputs—would be automated, with both automated and human checks throughout.

This would begin with data acquisition. Insofar as is possible, data could be programmatically read in from [application programming interfaces](https://aeturrell.com/blog/posts/the-prize-with-apis/the-prize-with-apis.html) (APIs) rather than sourced manually. This may not always be possible depending on the data source. But if the data are coming from, say, a survey, the information can still be acquired initially through digital means and validated at the very first stage. Surveys that are panels could be pre-populated with known answers to try and limit the fall off in responses due to "survey fatique."

![A schematic of the directed acyclic graph that a data orchestration tool would run.](survey_pipeline_dag.svg){width=60%}

The progression of data through a pipeline would be run by the data orchestration tools that are fairly common in production grade data science applications. Data orchestration tools schedule, monitor, and catch errors or issues in the processing of data by computer code. Orchestration flows can be triggered on on a schedule or according to an event, for example if a new file lands in a folder. They ensure that data are transformed, cleaned, and delivered efficiently and accurately. Orchestration tools can handle large volumes of data, support various data formats and programming languages, and can make it easier for organisations to monitor their entire estate of processes. Some popular data orchestration tools include [Apache Airflow](https://airflow.apache.org/) (first generation) and [Dagster](https://docs.dagster.io/) and [Prefect](https://docs.prefect.io/v3/get-started/index) (both second generation, with more advanced features). All of these are free and open source. For hosting of orchestration pipelines, [Google Cloud Composer](https://cloud.google.com/composer/docs/composer-3/composer-overview) and [Amazon Managed Workflows for Airflow](https://aws.amazon.com/managed-workflows-for-apache-airflow/) are both managed Airflow instances that can be configured using a dedicated API or using [Terraform](https://developer.hashicorp.com/terraform).

As data are passed around the workflows, they can be subject to checks that will raise a red flag should a problem be detected. "Type checking" would flag any issues with the format of the data. This can be achieved with data validation tools such as [Great Expectations](https://greatexpectations.io/), [Pandera](https://pandera.readthedocs.io/), and (for unstructured data) [Pydantic](https://docs.pydantic.dev/). Data validation and plausibility checks could look to see whether the data are consistent with historical and other contextual information too—this helps ensure that implausible values are caught early, including at the aggregate level.

Once data and pipelines are in the cloud and automated, it becomes possible to do data lineage tracking. This would allow one to understand how data change from landing, through processing and transformation, and to reports, applications, and statistics. Such lineage tracking can trace an error's origin, highlight inconsistencies across processes, and show what the downstream consequences of an error might be at the start of a pipeline. Data lineage tracking also enables sensitivity analysis, so that producers can ask questions like, "What would the statistic look like if this or that data point were incorrect?" This gives a sense of whether the overall numbers are driven by one or two responses that, if incorrect, could make the final numbers misleading.

Any changes to critical production-of-stats code would be subject to review, and to tests that would show how the output statistic would change subject to the code change.

At the end of the pipeline, the single source of truth on the final output would be an API, and data available in the traditional way (ie where it could be downloaded manually from a webpage) would be built on top of that API—ensuring that these sources were consistent and that the statistics could be accessed programmatically by others. APIs are the bedrock of online services and are already provided by a wide range of public sector organisations such as [Transport for London](https://api.tfl.gov.uk/) (TfL), the Federal Reserve Bank of St Louis' statistical service [FRED](https://fred.stlouisfed.org/), the [World Bank](https://datahelpdesk.worldbank.org/knowledgebase/articles/889386-developer-information-overview), and the [OECD](https://www.oecd.org/en/data/insights/data-explainers/2024/09/api.html). They are powerful because they allow for integration of downstream services: analysts using the statistics could write their own automated pipelines that directly consume relevant data feeds and businesses could use the data in services they provide (much as CityMapper does with the TfL API.)

Internally, dashboards of KPIs of the quality of statistics and code, and the numbers of manual and programmatic downloads of statistics, and other relevant pipeline data would be produced and displayed as part of the process—more on that in the management section!

## Skills and labour {#labour}

Recognising how technology is [changing the demands on workforces](https://www.ft.com/content/8e730692-fd9c-45b1-84dc-7ea16429c5c6), and the high costs of co-ordination of lots of people, you'd probably want to go for a leaner, more highly skilled and better paid workforce than has hitherto been the case. Fewer, more expert staff can be more agile and make risk-reward trade-offs more succesfully.

Fundamentally, the production of statistics is a technocratic endeavour, and you would want a workforce that reflects this: you'd want them to be highly skilled in relevant technical subject areas; for example, data science, statistics, and economics. The level of knowledge that you would want to see staff demonstrate in these areas would be high—so high that, like those similarly technocratic institutions, central banks, you would want to have a dedicated career offering for people with PhDs in relevant topics and you would make space for them to do research on how to make the statistics better.

To ensure that the workforce did meet these criteria, and to reflect that the use of code, data, and AI would be foundational to the tasks, you might want to require technical staff (most staff) to pass a coding test in Python and SQL.

Inevitably, going for a highly skilled workforce would mean needing to offer high salaries too—though pay would not need to be as high as in the private sector because access to the best data in the country would be a strong draw for economists, data scientists, and statisticians alike, and public service is a great motivator for many. To ensure that the overall budget remained sensible, one would have to have a smaller workforce however.

Getting the right skills is the most important part of any endeavour, and where you site your statistical production should reflect this: it's got to be where the people you want to work with are. Ideally, you would use a cost-benefit analysis of different locations and look at whether they could supply suitable labour at reasonable cost. A place with a thick labour market for highly educated graduates and post-graduates, but not too high a cost of living, is most likely to deliver this.

Once highly skilled labour is acquired, it would need to be cultivated and retained. Ongoing investment in human capital is all the more necessary in a world where AI means work is changing rapidly. Beyond that, there would need to be a compelling career offering for those who can bring value not just via managerial and organisational skills, but also via technical knowledge, ability to innovate, and professional leadership. Therefore, it would be essential to have technical career paths on offer. Technical career paths are common in frontier technology firms that require somewhat similar skills: Google has a complete parallel technical career ladder for roles up to and including Senior Vice President; Amazon, Meta, AirBnB, Uber, Apple, and Microsoft have similar pathways. This approach is not unprecedented in the public sector.

One advantage of having research skills would be to invigorate innovation. One (extremely imperfect) metric of innovation in statistics is how many research papers are published by national statistical organisations. As you can see in @fig-research, world-leading institutes like Statistics Netherlands and Statistics Canada fare well on this measure.

![A crude measure of innovation for selected institutions. INSEE is the French statistical agency. The Federal Reserve Bank of St Louis publishes a wide range of statistics through its [Federal Reserve Economic Statistics website](https://fred.stlouisfed.org/). Data from SCOPUS.](mean_papers_per_year.png){#fig-research}

It is not just highly skilled technical workers that will be required: management and leadership must also be highly skilled in their crafts. It almost goes without saying that poor leadership and bad management practices can undermine any endeavour, no matter how good the rest of the people working on it may be. Therefore, as well as seeking highly skilled technical staff, it is important that senior leadership have vision, the pragmatism to deliver, and the ability to implement the best practices in management. [Firms with better management practices have higher labour productivity](https://www.productivity.ac.uk/news/do-managers-matter/); why should we expect the production of statistics to be any different? And the benefits are significant: moving from the median to the 75th percentile of the distribution of management practices scores increases productivity by 11%. On the topic of management practices...

## Management practices, culture, and organisation

The best practices for culture, organisation, and management would need to be used to deliver the best possible statistics. Drawing from sources like [Do managers matter?](https://www.productivity.ac.uk/news/do-managers-matter/) and the techniques implemented at Amazon and others, the following innovations would be used:

- A culture of continuous improvement *within* existing teams. No hiving off improvement work to be done in a different department or team—innovation is everyone's job! Those improvements can be encouraged, monitored, and celebrated through KPIs...
- The use of Key Performance Indicators (KPIs) for tracking progress and for making key management decisions (eg around prioritisation.) These should be available through the whole organisation, and will help hold people to account for the statistics they are producing.
- Small, highly autonomous teams[^7] responsible for elements within the vertically integrated product-structure. Small teams operating with high autonomy within guardrails can help tackle co-ordination costs, remove the need for wide consensus (which leads to inaction), and prevents the diffusion of accountability. It is also much more empowering, which is strongly linked to job satisfaction.
- The use of targets that are stretching, tracked, and reviewed.
- Strict, rapid, and effective performance management.
- An organisational structure that reflects products and outputs rather than functional areas. This means that an entire production pipeline, from survey to final published statistics, will be owned by one area that can be held accountable for it. This is a contrast to having surveys in one department, technology in another, HR another, and statistics processing another. Functional models struggle to ensure aligned incentives. For example, if you have someone responsible for IT they are typically incentivised to minimise how much is spent on technology and to de-risk that technology as much as possible. However, another person who is responsible for processing statistics has quite different incentives: they want analysis to be done quickly (so prefer more expensive IT) and they are willing to take on some risk to get the job done. It can be hard for an organisation to solve this trade-off effectively, especially if these two people have reporting lines that only meet in the head of the organisation, who will have many other concerns on their plate. Instead, a product-oriented structure sees much of technology, HR, etc., embedded right next to the coal face.
- Similarly, "single-threaded" leadership, which means a single person is ultimately responsible for getting a product out. This leader is incentivised to make the optimal trade-off on risk and spend versus delivery. There is clear accountability in this model. Single-threaded means that leaders have end-to-end accountability for outcomes, decision bottlenecks are removed, and everyone under that leader is aligned on the goal.

- Kanban boards for management of work tasks, wherein everyone in the team knows what is happening, and a large number of routine updates are no longer necessary
- Asynchronous working patterns, such as status updates being automatic rather than through meetings (as above.) Synchronous meetings would be reserved for bilaterals and low-latency collaboration on complex issues. Documentation would be used obsessively for code, processes, decisions, and actions (this also lowers initial onboarding costs.) In particular, the documentation and code that processes data in a part of the pipeline will sit together and automated checks will ensure that they do not diverge (a common practice in high end software development.)
- Proposals to be discussed by committees in prose, not PowerPoint, in the form of notes with a page limit. Meetings would start with "silent time" so that everyone gets on the same page (literally) and can have a constructive discussion.
- Decisions counter-signed by all, and the reason for the decision documented.
- ["working backwards"](https://www.aboutamazon.com/news/workplace/an-insider-look-at-amazons-culture-and-processes) for new products
- ["disagree and commit"](https://en.wikipedia.org/wiki/Disagree_and_commit), to avoid consensus bias
- a single source of truth for all information

[^7]: Usually defined as those that can be fed with two pizzas.

## Conclusion

Our current statistical infrastructure is creaking dangerously. Could this crisis present an opportunity? We now have the tools—AI, cloud computing, modern data science, and battle-tested management practices from the world's most successful tech companies—to build anew.

Starting from scratch would mean **putting technology first**: automated pipelines running on secure cloud infrastructure, with AI catching errors that humans might miss, and APIs making statistics as accessible as checking the weather on your phone. It would mean **talent over scale**: a smaller, highly skilled workforce of data scientists, statisticians, and economists who can programme this technology to produce the best stats possible. And it would mean **innovation in management**: product-focused teams with single-threaded leadership, clear KPIs, and the management practices that have made Amazon successful, applied to the considerably more important task of measuring the economy.

Here's what should keep us all awake at night: we could be making billion-pound decisions based on statistics produced with clipboard-and-Excel methods while the tools to do this properly are sitting right there, proven and ready to deploy. The question isn't whether we *can* rebuild our statistical infrastructure for the modern age—the question is whether we will—and whether it can happen before the next crisis in the nation's numbers hits the headlines.

The blueprint exists. The technology works. The management practices are proven. And we *need* statistics fit for the choices that will shape Britain's future: because a decision based on the wrong data is like a house built on sand.
