{
  "hash": "d0bbe0e3c05aeaf9e10f1311360f5018",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: \"2023-10-13\"\nlayout: post\ntitle: \"Why have a model registry?\"\ncategories: [code, open-source, cloud, data science]\n---\n\n\n\n\nMany large institutions, including in the public sector, have a set of forecasts, predictions, or estimated statistical relationships (perhaps from a linear regression), that are key to their operations. In this post, I'll run through how these institutions might benefit from a *model registry* of the kind that more digitally-savvy frontier firms are already using. And why, without one, an institution might be running *model risk* without even realising it.\n\nIf you're not familiar with the idea of a *model registry*, it's a service offered by all three major cloud providers ([Amazon Web Services](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html), [Google Cloud Platform](https://cloud.google.com/vertex-ai/docs/model-registry/introduction), and [Microsoft Azure Platform](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-models)) that allows you to lodge models in a (private) repository online. You can also custom build your own model registry (and much more) using the wonderful open source package [MLflow](https://mlflow.org/docs/latest).\n\n## What even is a model?\n\nThe word \"model\" is doing a lot of work here: it means a digital artefact that is (usually) trained on some data and which either contains coefficient estimates or can make predictions or both. A linear regression is an example of a model. In the data science world, it would usually mean a trained machine learning model that can make new predictions given new inputs. Perhaps the most commonly used file format for models is pickle, with extension `.pkl`.[^1]\n\n[^1]: Of course, I'm not naïve, I know people are out there building models in Excel. What I really mean is \"the file format most commonly used by data professionals for models is pickle\". Today, almost undoubtedly, `.xlsx` is the most common file format for models but we'll see why that's not satisfactory.\n\nWant to see an example of a simple model in code, and the command to save it to disk? Scroll down or [click here](#a-simple-model-in-code-as-an-example).\n\n## What is a *model registry* and why do I need it?\n\nIf you've never come across one before, you can think of a model registry as a way to store, monitor, and log models and their properties. The pithiest description is that model registries do for models what GitHub and version control do for code. This metaphor goes quite far, as model registries also support a kind of [continuous integration](https://wandb.ai/hamelsmu/model-registry/reports/What-is-CI-CD-for-Machine-Learning---VmlldzozNzk2NDg2) where performance metrics can be generated automatically on model upload.\n\nYou can easily imagine a situation where this could be helpful. Let's say you're working with a model that you've trained locally on your computer. Let's also say that this model is critical to some decision, analysis, or operation that is happening in your institution. You might need to be able to collaborate with others on it: perhaps they will update it, perhaps they need to use it to make a prediction, or perhaps they just need to know that it exists because your institution is serious about keeping track of the critical models that are in use. One way of working is that the person who built the model does all of this, but then you're squeezing everything through a key person if you do that—it could easily lead to problems. So, okay, you could share the model via a network drive or send it over email, but there are a bunch of problems with this too. Unless you're sending it to everyone in the institution, email is pretty ineffective, and everyone is bad at manually doing version control through naming on a network drive. Plus, you have to find a way to make consistent model meta-data (eg on the pros and cons of that modelling approach) stay with the model file: good luck with that on a shared network drive!\n\nModel registries solve the problem of securely storing, monitoring, tracking, and sharing models between people. The schematic below, from [this blog post](https://levelup.gitconnected.com/everything-you-need-to-know-about-model-registry-f7b978e84a1), gives a sense of what their components are. Note that \"endpoints\" are places where the model gets deployed such that it can make predictions. This could be an API, but another endpoint could simply be users downloading them and running them locally.\n\n![Schematic of a model registry.](model_reg_img.png){width=100% fig-alt=\"Schematic of a model registry.\"}\n\nReasons you might want to use a model registry include:\n\n- you want to have a central set of approved models that are logged. No more tracking models separately and inconsistently, via, say a spreadsheet—in a model registry, the log of models *is* the set of models.\n- you want to have meta-data about the model that is entwined with the model. Some basic meta-data, like the format of inputs and outputs, and even on performance, is usually included. This can be extended with information on a model's limitations, strengths, and performance via something called a [model card](https://huggingface.co/docs/hub/model-cards). For example, you can [link model cards with models in the Amazon SageMaker model registry](https://aws.amazon.com/blogs/machine-learning/integrate-amazon-sagemaker-model-cards-with-the-model-registry/). You can [see an example model card for one of Google's models here](https://modelcards.withgoogle.com/object-detection#overview). Model cards are a bit new but there is progress toward a consistent schema [in this toolkit](https://github.com/tensorflow/model-card-toolkit), which has an example based on classifying incomes in the census.\n- you want to be able to reproduce model results. Version controlling models in a registry buys you this.\n- you want to be able to know what you would have predicted in the past. For models, reproducibility across time can be essential to an institution: it's not too much of a stretch to imagine there's an enquiry or review where it is necessary to reproduce the exact model output that you had from a particular date.\n- you want to track the performance of a model over time. This is another advantage of versioning, and makes a task like assessing the quality of forecasts over time go from being a rum game of chasing down people, emails, and files to being as simple as running a short script.\n- you want others to be able to use your models. If there's another team who rely on running your models, they no longer have to email you to ask—they just grab the latest version from the registry, and run it themselves. (Alternatively, the model [can be deployed to an API](../build-a-cloud-api/build-a-cloud-api.qmd) where anyone in the institution can ping it for a prediction.)\n- you want to control access to the model. Sometimes, institutions need to provide different levels of access to different models, and model registries make this very possible, and at as granular a level as you like.\n- you want to make your models discoverable. This is a killer application. Models on network drives are not very discoverable, and their meta-data even less so. Model registries are designed to help people find the model they need.\n- you want to know how many people are using models, or do other auditing. Model registries are typically hosted, and the hosting service counts what is happening.\n- you want to automate reports about the number of models, their performance, how often they are used or updated, and so on. Once code is involved, everything becomes automatable!\n- you want to host models for free! Yes, just storing models in a registry is free for most cloud providers. (Other actions, such as deploying models to endpoints, do cost though.)\n- you want to make sure that you're all using the same model. When you load a model from a registry, you pull it *directly* from the cloud repository—so there's no chance of you accidentally changing a version that you've downloaded to your own machine and running something that's inconsistent with the online version.\n\n*Not* having some of these features comes with significant *model risk*. By model risk, I mean risks relating to models such as not knowing which model or model version was used in a particular decision or at a particular time, losing track of key models, people having access to sensitive models that they shouldn't, not realising that a model's performance has significantly degraded, and being unable to reproduce results that were used in critical decision making.\n\nI said I would come back to models in Excel. In principle, you might (?) be able to store Excel models in a model registry and get some of these benefits (some, but definitely not all: you can't deploy Excel to an API endpoint, for example, or use it with continuous integration). But Excel comes with so many other issues, I really don't think you should. Some of the big ones are that they bundle code and data and model together, so you lose all the (not addressed in this post) benefits of version control, you lose control of data (very explicitly—[Excel changes your data](https://www.nature.com/articles/d41586-021-02211-4)), and the code is repeated across cells, which makes some errors more likely.[^2] The bottom line is that if the model is doing anything important, it shouldn't be in a spreadsheet.\n\n[^2]: The classic examples are not pulling a formula down to cover all rows or columns and hard-pasting numbers over formula cells.\n\n## Conclusion\n\nIf your institution relies on models in any way that's even vaguely critical, it's hard to see why you wouldn't want most of the functionality of a model registry or the attendant reduction in risks it can provide. Additionally, this is pretty cheap (or even free), and, because cloud services are scriptable, very automatable, so it's plausible that the extra functionality comes along with a *boost* in productivity. Model behaviour indeed!\n\n## Technical Appendix\n\n### A simple model in code as an example\n\nThe simplest model you can think of is something like the below, where we feed some data on the miles per gallon of cars into a regression.\n\n::: {#72a1c4e8 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\nmpg = pd.read_csv(\n    \"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv\",\n    dtype={\"model\": str, \"mpg\": float, \"hp\": float, \"disp\": float, \"cyl\": \"category\", \"wt\": float},\n)\nmpg.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rownames</th>\n      <th>mpg</th>\n      <th>cyl</th>\n      <th>disp</th>\n      <th>hp</th>\n      <th>drat</th>\n      <th>wt</th>\n      <th>qsec</th>\n      <th>vs</th>\n      <th>am</th>\n      <th>gear</th>\n      <th>carb</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Mazda RX4</td>\n      <td>21.0</td>\n      <td>6</td>\n      <td>160.0</td>\n      <td>110.0</td>\n      <td>3.90</td>\n      <td>2.620</td>\n      <td>16.46</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Mazda RX4 Wag</td>\n      <td>21.0</td>\n      <td>6</td>\n      <td>160.0</td>\n      <td>110.0</td>\n      <td>3.90</td>\n      <td>2.875</td>\n      <td>17.02</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Datsun 710</td>\n      <td>22.8</td>\n      <td>4</td>\n      <td>108.0</td>\n      <td>93.0</td>\n      <td>3.85</td>\n      <td>2.320</td>\n      <td>18.61</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Hornet 4 Drive</td>\n      <td>21.4</td>\n      <td>6</td>\n      <td>258.0</td>\n      <td>110.0</td>\n      <td>3.08</td>\n      <td>3.215</td>\n      <td>19.44</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hornet Sportabout</td>\n      <td>18.7</td>\n      <td>8</td>\n      <td>360.0</td>\n      <td>175.0</td>\n      <td>3.15</td>\n      <td>3.440</td>\n      <td>17.02</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNow we can fit a linear model, `lin_model`.\n\n::: {#96b111a2 .cell execution_count=2}\n``` {.python .cell-code}\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nlin_model = smf.ols(\"mpg ~ hp + C(cyl) + wt -1\", data=mpg).fit()\n```\n:::\n\n\nThis has a predict method, accessed via `lin_model.predict()`. \n\nOkay, *here's the important bit*: to save this model locally on your computer (ready to upload to a model registru), it would be `lin_model.save(\"lin_reg_model.pkl\")`.\n\nFor this particular package, **statsmodels**, you can see a summary of the underlying model using `.summary()`\n\n::: {#a72c7761 .cell execution_count=3}\n``` {.python .cell-code}\nlin_model.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.857</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.836</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   40.53</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Tue, 10 Dec 2024</td> <th>  Prob (F-statistic):</th> <td>4.87e-11</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>12:21:31</td>     <th>  Log-Likelihood:    </th> <td> -71.235</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    32</td>      <th>  AIC:               </th> <td>   152.5</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    27</td>      <th>  BIC:               </th> <td>   159.8</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>C(cyl)[4]</th> <td>   35.8460</td> <td>    2.041</td> <td>   17.563</td> <td> 0.000</td> <td>   31.658</td> <td>   40.034</td>\n</tr>\n<tr>\n  <th>C(cyl)[6]</th> <td>   32.4870</td> <td>    2.811</td> <td>   11.555</td> <td> 0.000</td> <td>   26.718</td> <td>   38.256</td>\n</tr>\n<tr>\n  <th>C(cyl)[8]</th> <td>   32.6601</td> <td>    3.835</td> <td>    8.516</td> <td> 0.000</td> <td>   24.791</td> <td>   40.530</td>\n</tr>\n<tr>\n  <th>hp</th>        <td>   -0.0231</td> <td>    0.012</td> <td>   -1.934</td> <td> 0.064</td> <td>   -0.048</td> <td>    0.001</td>\n</tr>\n<tr>\n  <th>wt</th>        <td>   -3.1814</td> <td>    0.720</td> <td>   -4.421</td> <td> 0.000</td> <td>   -4.658</td> <td>   -1.705</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 2.972</td> <th>  Durbin-Watson:     </th> <td>   1.790</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.226</td> <th>  Jarque-Bera (JB):  </th> <td>   1.864</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td> 0.569</td> <th>  Prob(JB):          </th> <td>   0.394</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 3.320</td> <th>  Cond. No.          </th> <td>1.90e+03</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.9e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.\n```\n:::\n:::\n\n\n### Creating and working with a model registry\n\nIt's simpler than you might think to get started with a model registry; you can find the instructions for [Google Cloud's Vertex AI model registry here](https://cloud.google.com/vertex-ai/docs/start/cloud-environment), for [Amazon's SageMaker registry here](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html), for [Microsoft's Azure Machine Learning here](https://learn.microsoft.com/en-us/azure/machine-learning/concept-machine-learning-registries-mlops?view=azureml-api-2), and for the [MLflow model registry here](https://mlflow.org/docs/latest/model-registry.html#).\n\nOnce you've created a model registry, you can upload a model in a number of ways. Most providers give you at least three ways:\n\n- through a user interface, manually\n- via a command line interface\n- via a Python package\n\nFor example, once you've done the initial settings, [uploading a model to Azure's model registry](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-models?view=azureml-api-2&tabs=python%2Cuse-local#register-your-model-as-an-asset-in-machine-learning-by-using-the-sdk) is achieved using their Python package via\n\n```python\nfrom azure.ai.ml.entities import Model\nfrom azure.ai.ml.constants import AssetTypes\n\nfile_model = Model(\n    path=\"model.pkl\",\n    type=AssetTypes.CUSTOM_MODEL,\n    name=\"local-file-example\",\n    description=\"Model created from local file.\",\n)\nml_client.models.create_or_update(file_model)\n```\n\nAlthough you can search for models in the user interface (see [below](#an-example-of-a-model-registry) for an example from Google), you can also search all models programmatically too. For example, in Azure, this is as simple as `client.search_registered_models(order_by=[\"name ASC\"])`.\n\nLoading models from the registry into your local workspace is similarly simple using, say, MLflow on Azure: `model = mlflow.pyfunc.load_model(f\"models:/{model_name}/Staging\")`. You can also deploy models online to an endpoint and query that too, and this is how the cloud providers typically assume you will use registered models.\n\n#### Model cards\n\nIt's fair to say that Amazon is ahead on model cards, with Google not having introduced them on their cloud service yet and Microsoft offering something that's slightly different. You can [find out how to link models on Amazon with model cards here for AWS](https://aws.amazon.com/blogs/machine-learning/integrate-amazon-sagemaker-model-cards-with-the-model-registry/), and, as you'd expect, there's a way to [programmatically upload this info using Python](https://docs.aws.amazon.com/sagemaker/latest/dg/model-cards-create.html).\n\n## An example of a model registry\n\nThe image below shows a view of a model registry in Google's model registry, Vertex AI.\n\n![Screenshot showing the model registry page on Vertex AI.](list_view_google_mr.jpg){width=100% fig-alt=\"A Google model registry with some models in.\"}\n\nYou can see high level information such as the name, number of versions, default version, deployment status, and type. If you click through to one of those models, you get a more detailed view.\n\n![Screenshot showing a single model's page in a Google model registry.](single_model_view_google_mr.jpg){width=100% fig-alt=\"A single model's page in a Google model registry.\"}\n\nThis view shows the labels applied to each version of a specific model, and more. One of the great features of a model registry is that performance can be automatically assessed (\"continous integration\").\n\nThe image below shows an example of some metrics of this that live *within* the model registry:\n\n![Screenshot showing a single model's performance metrics in Google model registry.](performance_view_google_mr.jpg){width=100% fig-alt=\"A single model's performance metrics in Google model registry.\"}\n\n### Metadata\n\nJust as an example, here are some meta-data that might be associated with a model \n\n```text\nartifact_path: classifier\nflavors:\n  fastai:\n    data: model.fastai\n    fastai_version: 2.4.1\n  python_function:\n    data: model.fastai\n    env: conda.yaml\n    loader_module: mlflow.fastai\n    python_version: 3.8.12\nmodel_uuid: e694c68eba484299976b06ab9058f636\nrun_id: e13da8ac-b1e6-45d4-a9b2-6a0a5cfac537\nsignature:\n  inputs: '[{\"type\": \"tensor\",\n             \"tensor-spec\": \n                 {\"dtype\": \"uint8\", \"shape\": [-1, 300, 300, 3]}\n           }]'\n  outputs: '[{\"type\": \"tensor\", \n              \"tensor-spec\": \n                 {\"dtype\": \"float32\", \"shape\": [-1,2]}\n            }]'\n```\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}