---
date: "2025-02-26"
layout: post
draft: true
title: "What does best practice look like for regular analytical processes in most institutions?"
categories: [productivity, analysis, public sector]
image: "../../../code/logos/cookiecutter_logo_package.svg"
---

If you work at a big institution, it's very likely that there are regular analytical processes that need to happen. What's the best practice for ensuring these run smoothly?

First, let's be clear what this mysterious thing, a *regular analytical process*, is. For me, it's something that you know you're going to need to run or produce on a schedule, and which involves some analysis---the typical example would be one where some new data arrives, is turned into various statistics (which could be as simple as a mean), and which are then output in some way; in a report, by email, perhaps uploaded to a website.

For national statistical institutes, this could be taking survey data and turning it into estimates of unemployment. For regulators, perhaps its a set of aggregate statistics on the firms or markets that they regulate. For health institutions, perhaps it's data on performance. For firms, it's likely to be key performance indicators.

Regardless of the context or statistics in question, the process splits into a few separate steps that occur sequentially

- Acquisition of data needed for statistics
- Processing data into statistics
- Dissemination of statistics

No doubt there are really fancy ways to do these steps. But most of the institutions I've mentioned are *not* frontier tech firms and don't have data engineers just itching to deploy to production-grade databases. So the focus here is going to be on best practices that are a bit more in reach for the average institution.

## Acquisition of data

