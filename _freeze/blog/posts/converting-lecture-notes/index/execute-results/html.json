{
  "hash": "020216329c1b8ae925b00ee73af920bf",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: \"2024-12-28\"\nlayout: post\ntitle: \"Converting handwritten, maths-heavy lecture notes to markdown using large language models\"\ncategories: [llm, python, data science]\nimage: lecture_output_gemini.webp\nexecute:\n  echo: false\njupyter: python3\n---\n\n\n\n\nIt would be nice to have digital copies of all of those old handwritten lecture notes that I so lovingly put together. Some of them might even still be useful, though I have to admit I don't have tons of opportunities to use quantum field theory these days.\n\nRecent Large Language Models (LLMs) have stunning vision capabilities and so it occurred to me that they might be able to convert even old notes into beautifully formatted markdown and equations.\n\n## Models\n\nI'll try out two recent models from OpenAI and Google respectively,\n\n- chatgpt-4o-latest\n- gemini-exp-1206\n\nThe latter has a generous free tier, the former cost $0.03 to run one image.\n\n## Code\n\nThe Python APIs for both of these couldn't really be simpler—you can find the example [in this repo](https://github.com/aeturrell/digitise-lecture-notes), but, to give you a sense, here is the complete code to use Google's Gemini model:\n\n```python\nimport os\nfrom pathlib import Path\n\nimport google.generativeai as genai\nimport PIL.Image\nfrom dotenv import load_dotenv\n\nload_dotenv()\ngenai.configure(api_key=os.getenv(\"API_KEY_GOOGLE\"))\n\n# Path to image\nimage_path = Path(\"input/test_lecture_note_photo.jpg\")\nsample_file = PIL.Image.open(image_path)\n\n# Choose a Gemini model.\nmodel = genai.GenerativeModel(model_name=\"gemini-exp-1206\")\n\nprompt = \"The photograph shows some lecture notes. Please transcribe the text in this photograph to markdown, using latex for equations where needed.\"\n\nresponse = model.generate_content([prompt, sample_file])\n\nopen(Path(\"output/output_example_gemini.md\"), \"w\").write(response.text)\n```\n\n## Results\n\nThis is the input file. Yes, I took a bad photograph. In some ways, that's quite helpful, as it gives a sense of how these models do in a difficult situation. If you were doing this at scale, you'd probably want to find a consistent way of taking high quality photographs, however.\n\n![Example input of handwritten lecture note](test_lecture_note.webp)\n\nAnd here are the outputs:\n\n![Output capture from GPT-4o](lecture_output_gpt4o.webp)\n\n![Output capture from gemini-exp-1206](lecture_output_gemini.webp)\n\nThe GPT-4o model cost around $0.03, while the experimental Gemini model is free up to so many tokens per day. They both do a really great job given the quality of the input and you can see that this would probably work at scale with some better photographs.[^1] They each make a couple of mistakes, and Gemini seems more keen on beautiful latex equations than it does on markdown formatting, at least relative to GPT-4o. Bear in mind, there was close to zero effort expended on designing the prompt here too—so it's likely that this could be improved too.\n\nAll in all, this looks completely feasible. And there's some speculation (h/t: [Simon Willison](https://simonwillison.net/)) that vision models that can run *locally* (Llama 3.2, Qwen-VL, and Pixtral) would be able to handle it too.\n\n[^1]: Garbage in, garbage out.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}